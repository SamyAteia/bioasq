{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "zREWLyX7Aq1P"
   },
   "source": [
    "# Phase A \n",
    "Question in -> List of max 10 articles out; List of 10 relevant snippets out.\n",
    "\n",
    "Components:\n",
    "-> Question to PubMed? Query \n",
    "-> Relevancy ranking\n",
    "-> Article snippet candidate extractor (Only from Abstract and Title?)\n",
    "-> Snippet ranking\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "%pip install Biopython "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yvwBOiW8btpe"
   },
   "source": [
    "## Initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kuEZPOu1bwru"
   },
   "outputs": [],
   "source": [
    "# Define base URL for OpenAI API\n",
    "base_url = \"https://api.openai.com/v1/\"\n",
    "\n",
    "# Define your API key (replace with your own)\n",
    "api_key = input()\n",
    "\n",
    "# Define headers for authorization and organization\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"OpenAI-Organization\": None # replace with your own\n",
    "}\n",
    "\n",
    "#model_name = \"gpt-4-0314\"\n",
    "#model_name = \"gpt-3.5-turbo-0301\"\n",
    "model_name = \"gpt-3.5-turbo\"\n",
    "#model_name = \"gpt-4\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "XOkj8pGeALMc",
    "tags": []
   },
   "source": [
    "## Query Expansion Module"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "HZ70S6rgtrNJ"
   },
   "source": [
    "### Query Expansion V2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "id": "HZ70S6rgtrNJ"
   },
   "outputs": [],
   "source": [
    "# Import requests library\n",
    "# Query Expansion V2\n",
    "import requests\n",
    "\n",
    "def expand_query(question):\n",
    "    \n",
    "    # Define parameters for chat completion\n",
    "    params_chat = {\n",
    "        \"model\": model_name, # model name\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, research, and information retrieval in the biomedical domain.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Expand this search query: '{question}' for PubMed by incorporating synonyms and additional terms that closely relate to \\\n",
    "            the main topic and help reduce ambiguity. Assume that phrases are not stemmed; therefore, generate useful variations. Return only the query that can \\\n",
    "            directly be used without any explanation text. Focus on maintaining the query's precision and relevance to the original question.\"}\n",
    "        ], \n",
    "        \"temperature\": 0.0, # randomness of completion\n",
    "        \"frequency_penalty\": 0.5, # discourage repetition of words or phrases\n",
    "        \"presence_penalty\": 0.1, # discourage new topics or entities\n",
    "    }\n",
    "\n",
    "    # Make request to chat completion\n",
    "    response_chat = requests.post(base_url + \"chat/completions\", headers=headers, json=params_chat)\n",
    "\n",
    "    # Check status code\n",
    "    if response_chat.status_code == 200:\n",
    "        # Parse response as JSON\n",
    "        data_chat = response_chat.json()\n",
    "\n",
    "        # Get the generated message from the chat completion result\n",
    "        generated_message = data_chat[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # Return the generated message\n",
    "        return generated_message\n",
    "\n",
    "    else:\n",
    "        print(response_chat.text)\n",
    "        raise Exception(f\"Error: {response_chat.status_code}\")\n",
    "\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O2HG828YlXn_",
    "tags": []
   },
   "source": [
    "## Refinment Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def refine_query(question, original_query):\n",
    "    params_chat = {\n",
    "        \"model\": model_name, # model name\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, \\\n",
    "             research, and information retrieval in the biomedical domain.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"Given that the following search query for PubMed has returned\\\n",
    "            no documents, please generate a broader query that retains the original question's context and relevance.\\\n",
    "            Assume that phrases are not stemmed; therefore, generate useful variations. Return only the query that can\\\n",
    "            directly be used without any explanation text. Focus on maintaining the query's precision and relevance to\\\n",
    "            the original question. Original question: '{question}', Original query: '{original_query}'.\"}\n",
    "        ],\n",
    "        \"temperature\": 0.0, # randomness of completion\n",
    "        \"frequency_penalty\": 0.6, # discourage repetition of words or phrases\n",
    "        \"presence_penalty\": 0.2, # discourage new topics or entities\n",
    "    }\n",
    "    print(params_chat)\n",
    "\n",
    "     # Make request to chat completion\n",
    "    response_chat = requests.post(base_url + \"chat/completions\", headers=headers, json=params_chat)\n",
    "\n",
    "    # Check status code\n",
    "    if response_chat.status_code == 200:\n",
    "        # Parse response as JSON\n",
    "        data_chat = response_chat.json()\n",
    "\n",
    "        # Get the generated message from the chat completion result\n",
    "        generated_message = data_chat[\"choices\"][0][\"message\"][\"content\"]\n",
    "\n",
    "        # Return the generated message\n",
    "        return generated_message\n",
    "\n",
    "    else:\n",
    "        print(response_chat.text)\n",
    "        raise Exception(f\"Error: {response_chat.status_code}\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "O2HG828YlXn_",
    "tags": []
   },
   "source": [
    "## Reranking Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "id": "m79V_ZDTlahv"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "\n",
    "def rerank_top_articles(question, articles):\n",
    "\n",
    "    # Prepare the list of articles as a single string for the message\n",
    "    articles_str = \"\\n\".join([f\"{idx + 1} - {article['title']}\" for idx, article in enumerate(articles)])\n",
    "    nr_of_articles = min(10, len(articles))\n",
    "    \n",
    "\n",
    "    # Define parameters for chat completion\n",
    "    params_chat = {\n",
    "        \"model\": model_name, # model name\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, \\\n",
    "            research, and information retrieval in the biomedical domain.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{articles_str} \\n\\n Given these articles and the question: '{question}'. \\\n",
    "             Rerank the articles based on their relevance to the question and return the top {nr_of_articles} most relevant articles as a comma seperated list of their index ids. Don't explain your answer, return only this list, for example: '1, 2, 3, 4' \"}\n",
    "        ],\n",
    "        \"temperature\": 0.0, # randomness of completion\n",
    "        \"frequency_penalty\": 0.3, # discourage repetition of words or phrases\n",
    "        \"presence_penalty\": 0.1, # discourage new topics or entities\n",
    "    }\n",
    "    # Make request to chat completion\n",
    "    response_chat = requests.post(base_url + \"chat/completions\", headers=headers, json=params_chat)\n",
    "\n",
    "    # Check status code\n",
    "    if response_chat.status_code == 200:\n",
    "        # Parse response as JSON\n",
    "        data_chat = response_chat.json()\n",
    "\n",
    "        # Get the generated message from the chat completion result\n",
    "        generated_message = data_chat[\"choices\"][0][\"message\"][\"content\"]\n",
    "        print(\"generated rerank response: \")\n",
    "        print(generated_message)\n",
    "\n",
    "        # Extract the reranked article indices from the generated message\n",
    "        reranked_indices = [int(x.strip()) - 1 for x in generated_message.split(',')]\n",
    "\n",
    "        # Rerank the articles based on the extracted indices\n",
    "        reranked_articles = [articles[idx] for idx in reranked_indices]\n",
    "\n",
    "        # Return the top 10 reranked articles\n",
    "        return reranked_articles[:10]\n",
    "\n",
    "    else:\n",
    "        print(response_chat.text)\n",
    "        raise Exception(f\"Error: {response_chat.status_code} {response_chat.reason} {response_chat.text}\")\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "l5a4ArKDuBc8",
    "tags": []
   },
   "source": [
    "## Snippet Generation Module"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "7bqdRLKcuHPy"
   },
   "outputs": [],
   "source": [
    "import requests\n",
    "import json\n",
    "\n",
    "\n",
    "def snippet_call_with_article_str(question, articles_str):\n",
    "    # Define parameters for chat completion\n",
    "    params_chat = {\n",
    "        \"model\": model_name, # model name\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert system in question answering, \\\n",
    "             research, and information retrieval in the biomedical domain. You only answer in JSON format.\"},\n",
    "            {\"role\": \"user\", \"content\": f\"{articles_str} \\n Given above list of articles and the question: '{question}'. \\\n",
    "             Extract the most relevant snippets for the question from title or abstract and return them in as a JSON list of JSON objects with the following structure.\\\n",
    "             Each snippet should become a json object containing the fields: document (the unique document url of the article from which the snippet is taken),\\\n",
    "             beginSection (either title or abstract), endSection (same as beginSection), \\\n",
    "             offsetInBeginSection (the character offset where the snippet starts), offsetInEndSection (the character offset where the snippet ends), \\\n",
    "             and text (the text of the snippet). Each snippet should be at most 3 sentences long, extract at most 3 snippets per article. The answer should be a valid JSON list. \\\n",
    "             \\n\\n\"}\n",
    "        ],\n",
    "        \"temperature\": 0.0, # randomness of completion\n",
    "        \"frequency_penalty\": 0.3, # discourage repetition of words or phrases\n",
    "        \"presence_penalty\": 0.1, # discourage new topics or entities\n",
    "    }\n",
    "\n",
    "    # Make request to chat completion\n",
    "    response_chat = requests.post(base_url + \"chat/completions\", headers=headers, json=params_chat)\n",
    "\n",
    "    # Check status code\n",
    "    if response_chat.status_code == 200:\n",
    "        # Parse response as JSON\n",
    "        data_chat = response_chat.json()\n",
    "\n",
    "        # Get the generated message from the chat completion result\n",
    "        generated_message = data_chat[\"choices\"][0][\"message\"][\"content\"]\n",
    "        print(\"gpt response:\")\n",
    "        print(generated_message)\n",
    "        \n",
    "        cleaned_message = generated_message.replace(\"\\\\xa0\", \" \").replace(\"\\xa0\", \" \")\n",
    "\n",
    "        # Extract the relevant snippets from the generated message\n",
    "        relevant_snippets = json.loads(cleaned_message)\n",
    "\n",
    "        return relevant_snippets\n",
    "\n",
    "    else:\n",
    "        print(response_chat.text)\n",
    "        raise Exception(f\"Error: {response_chat.status_code}\")\n",
    "\n",
    "def extract_relevant_snippets(question, articles):\n",
    "\n",
    "    if len(articles) > 0:\n",
    "        # Generate a string for the first 3 articles or all articles if the list contains less than 3\n",
    "        articles_str_1 = \"\\n\".join([f\"{idx + 1}. {article['id']} - title:'{article['title']}' abstract:'{article['abstract']}'\" for idx, article in enumerate(articles[:min(3, len(articles))])])\n",
    "\n",
    "        # Generate a string for the next 3 articles only if the list contains more than 3 articles\n",
    "        if len(articles) > 3:\n",
    "            articles_str_2 = \"\\n\".join([f\"{idx + 4}. {article['id']} - title:'{article['title']}' abstract:'{article['abstract']}'\" for idx, article in enumerate(articles[3:6])])\n",
    "        else:\n",
    "            articles_str_2 = \"\"\n",
    "\n",
    "        print(\"First 3 articles or all if less than 3:\")\n",
    "        print(articles_str_1)\n",
    "        snippets = snippet_call_with_article_str(question, articles_str_1)\n",
    "        \n",
    "        if articles_str_2:\n",
    "            print(\"\\nNext 3 articles:\")\n",
    "            print(articles_str_2)\n",
    "            snippets.extend(snippet_call_with_article_str(question, articles_str_2))\n",
    "\n",
    "            # Generate a string for the last articles if the list contains more than 6 articles\n",
    "            if len(articles) > 6:\n",
    "                articles_str_3 = \"\\n\".join([f\"{idx + 7}. {article['id']} - title:'{article['title']}' abstract:'{article['abstract']}'\" for idx, article in enumerate(articles[6:])])\n",
    "                print(\"\\nLast articles:\")\n",
    "                print(articles_str_3)\n",
    "                snippets.extend(snippet_call_with_article_str(question, articles_str_3))\n",
    "        \n",
    "        return snippets\n",
    "    else:\n",
    "        return []\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "t7ufKqTCk0ZB",
    "tags": []
   },
   "source": [
    "## Retrieval"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "id": "LD3ID3vSkztQ"
   },
   "outputs": [],
   "source": [
    "from Bio import Entrez\n",
    "from urllib.error import HTTPError\n",
    "\n",
    "\n",
    "Entrez.email = None #replace with your own\n",
    "Entrez.api_key = None #replace with your own\n",
    "\n",
    "def create_article_dict(pmid, title, abstract):\n",
    "    if isinstance(abstract, list):\n",
    "        abstract = \" \".join([str(a) for a in abstract])\n",
    "\n",
    "    return {\n",
    "        \"id\": \"http://www.ncbi.nlm.nih.gov/pubmed/\" + pmid,\n",
    "        \"title\": title,\n",
    "        \"abstract\": abstract\n",
    "    }\n",
    "\n",
    "def get_pubmed_article_details(article_ids):\n",
    "    try:\n",
    "        handle = Entrez.efetch(db=\"pubmed\", id=\",\".join(article_ids), rettype=\"medline\", retmode=\"xml\")\n",
    "    except HTTPError as e:\n",
    "        print(e)\n",
    "        print(e.response.text)\n",
    "\n",
    "    records = Entrez.read(handle)\n",
    "    handle.close()\n",
    "\n",
    "    articles = []\n",
    "    for record in records['PubmedArticle']:\n",
    "        title = record['MedlineCitation']['Article']['ArticleTitle']\n",
    "        abstract = record['MedlineCitation']['Article'].get('Abstract', {}).get('AbstractText', None)\n",
    "        pmid = record['MedlineCitation']['PMID']\n",
    "        articles.append(create_article_dict(pmid, title, abstract))\n",
    "\n",
    "    for record in records['PubmedBookArticle']:\n",
    "        title = record['BookDocument']['Book']['BookTitle']\n",
    "        abstract = record['BookDocument'].get('Abstract', {}).get('AbstractText', None)\n",
    "        pmid = record['BookDocument']['PMID']\n",
    "        articles.append(create_article_dict(pmid, title, abstract))\n",
    "        \n",
    "    return articles\n",
    "\n",
    "\n",
    "def search_pubmed_and_bioasq(query, articles_per_page=50):\n",
    "    handle_esearch = Entrez.esearch(db=\"pubmed\", term=query, retmax=articles_per_page, sort=\"relevance\", datetype=\"pdat\", maxdate=\"2022/12/09\")\n",
    "    data_esearch = Entrez.read(handle_esearch)\n",
    "    id_list = data_esearch[\"IdList\"]\n",
    "    print(id_list)\n",
    "\n",
    "    articles = []\n",
    "    if len(id_list) > 0:\n",
    "        articles = get_pubmed_article_details(id_list)\n",
    "    return articles\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "KYJSyeeXcnY2",
    "tags": []
   },
   "source": [
    "## PhaseA Run"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 1000
    },
    "id": "6WiuUhk8cldL",
    "outputId": "ad08b580-737b-4fae-b121-88b39260b089"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import time  # Import the time module\n",
    "\n",
    "def append_to_logfile(logfile_name, text):\n",
    "    with open(logfile_name, 'a', encoding='utf-8') as logfile:\n",
    "        logfile.write(text + \"\\n\")\n",
    "\n",
    "# Get the current timestamp in a sortable format\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "logfile_name = f\"{timestamp}_{model_name}_PhaseA_No_Expansion_log_file.json\"\n",
    "debug_logfile = f\"{timestamp}_{model_name}_PhaseA_No_Expansion_Debug_log_file.json\"\n",
    "\n",
    "def get_relevant_articles(question):\n",
    "    # stub function for getting relevant articles\n",
    "    print(\"Question: \"+question)\n",
    "    append_to_logfile(debug_logfile, f\"{{\\\"question\\\":\\\"{question}\\\",\")\n",
    "    #query = expand_query(question);\n",
    "    query = question\n",
    "    print(\"Query: \"+query)\n",
    "    append_to_logfile(debug_logfile, f\"\\\"query\\\":\\\"{query}\\\"\")\n",
    "    relevant_articles = search_pubmed_and_bioasq(query)\n",
    "    #No Query expansion therefore no query refinement\n",
    "    #if len(relevant_articles) == 0:\n",
    "    #    print(\"Refining Query\")\n",
    "    #    query = refine_query(question, query)\n",
    "    #    print(\"refined query:\"+ query)\n",
    "    #   append_to_logfile(debug_logfile, f\"\\\"refined_query\\\":\\\"{query}\\\"\")\n",
    "    #    relevant_articles = search_pubmed_and_bioasq(query)\n",
    "    append_to_logfile(debug_logfile, f\"\\\"retrieved_articles\\\":{relevant_articles},\")\n",
    "    if len(relevant_articles) > 1:\n",
    "        top_articles = rerank_top_articles(question, relevant_articles)\n",
    "        append_to_logfile(debug_logfile, f\"\\\"reranked_articles\\\":{top_articles}}}\")\n",
    "    else:\n",
    "        top_articles = relevant_articles[:10]\n",
    "    return top_articles\n",
    "\n",
    "def get_relevant_snippets(articles, question):\n",
    "    # stub function for getting relevant snippets\n",
    "    relevant_snippets = extract_relevant_snippets(question, articles)\n",
    "    print(\"relevant snippets: \")\n",
    "    print(relevant_snippets)\n",
    "    return relevant_snippets\n",
    "\n",
    "# Load the input file in JSON format\n",
    "with open('./BioASQ-task11bPhaseA-testset4.json') as input_file:\n",
    "    data = json.loads(input_file.read())\n",
    "\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "append_to_logfile(debug_logfile, \"[\")\n",
    "\n",
    "\n",
    "# Iterate over all questions\n",
    "offset = 0\n",
    "for idx, question in enumerate(data[\"questions\"]):\n",
    "    print(idx)\n",
    "    if idx < offset:\n",
    "        continue\n",
    "\n",
    "    retry_count = 0  # Initialize a counter for retries\n",
    "    while retry_count < 2:  # Set the maximum number of retries to 2\n",
    "        try:\n",
    "            # Call the stub function to get relevant articles\n",
    "            relevant_articles = get_relevant_articles(question['body'])\n",
    "            relevant_articles_ids = [article['id'] for article in relevant_articles]\n",
    "            print(relevant_articles_ids)\n",
    "\n",
    "            # Call the stub function to get relevant snippets\n",
    "            #relevant_snippets = get_relevant_snippets(relevant_articles, question['body'])\n",
    "            relevant_snippets = []\n",
    "\n",
    "            # Create a dictionary to store the results for this question\n",
    "            question_results = {\n",
    "                \"body\": question[\"body\"],\n",
    "                \"id\": question[\"id\"],\n",
    "                \"type\": question[\"type\"],\n",
    "                \"documents\": relevant_articles_ids,\n",
    "                \"snippets\": relevant_snippets\n",
    "            }\n",
    "\n",
    "            append_to_logfile(logfile_name, json.dumps(question_results))\n",
    "\n",
    "            # Add the results for this question to the list of all results\n",
    "            results.append(question_results)\n",
    "            break  # If no exception is thrown, break the loop\n",
    "        except Exception as e:\n",
    "            print(f\"Error processing question {idx}: {e}\")\n",
    "            retry_count += 1  # Increment the retry counter\n",
    "            time.sleep(5)  # Sleep for 5 seconds before retrying\n",
    "\n",
    "# Create a dictionary to store the results for all questions\n",
    "output = {\n",
    "    \"questions\": results\n",
    "}\n",
    "\n",
    "append_to_logfile(debug_logfile, \"]\")\n",
    "\n",
    "\n",
    "# Prefix the output file name with the timestamp\n",
    "output_file_name = f\"./Result/{timestamp}_{model_name}_PhaseA_NoExpansion_output_file.json\"\n",
    "\n",
    "# Save the output to a file in pretty-formatted JSON format\n",
    "with open(output_file_name, \"w\") as f:\n",
    "    json.dump(output, f, indent=4)"
   ]
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
