{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "yvwBOiW8btpe"
   },
   "source": [
    "## Initial Parameters"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "id": "kuEZPOu1bwru"
   },
   "outputs": [],
   "source": [
    "# Define base URL for OpenAI API\n",
    "base_url = \"https://api.openai.com/v1/\"\n",
    "\n",
    "# Define your API key (replace with your own)\n",
    "api_key = input()\n",
    "\n",
    "# Define headers for authorization and organization\n",
    "headers = {\n",
    "    \"Authorization\": f\"Bearer {api_key}\",\n",
    "    \"OpenAI-Organization\": None # replace with your own\n",
    "}\n",
    "\n",
    "#model_name = \"gpt-4-0314\"\n",
    "#model_name = \"gpt-3.5-turbo-0301\"\n",
    "#model_name = \"gpt-3.5-turbo\"\n",
    "model_name = \"gpt-4\""
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "qMn-tUtVjQyo"
   },
   "source": [
    "# Phase B\n",
    "\n",
    "Phase B -> in Question + Article list + Snippets -> Ideal Answer + Exact Answers\n",
    "-> Ideal Answer generator \n",
    "-> Exact Answer generator\n",
    "\n",
    "Phase B Rules:\n",
    "-> Do not return synonyms\n",
    "-> Provided Articles should be enough\n",
    "-> Additional Articles can be used\n"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {
    "id": "P7fJIkR3jgIw",
    "jp-MarkdownHeadingCollapsed": true,
    "tags": []
   },
   "source": [
    "## Phase B Run\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "colab": {
     "base_uri": "https://localhost:8080/",
     "height": 73
    },
    "id": "9MfkBxiBjjJ3",
    "outputId": "5aaf6537-8003-4fde-dc27-6f18e11dfd09"
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import datetime\n",
    "import time  # Import the time module\n",
    "import requests\n",
    "import string\n",
    "\n",
    "\n",
    "def append_to_logfile(logfile_name, text):\n",
    "    with open(logfile_name, 'a') as logfile:\n",
    "        logfile.write(text + \"\\n\")\n",
    "\n",
    "def remove_punctuation_and_lowercase(text):\n",
    "    # Lowercase the string\n",
    "    text = text.lower()\n",
    "\n",
    "    # Remove punctuation\n",
    "    text = text.translate(str.maketrans(\"\", \"\", string.punctuation))\n",
    "\n",
    "    return text\n",
    "\n",
    "def generate_exact_answer(question, articles, snippets):\n",
    "    # stub function for generating exact answer\n",
    "    exact_answer = []\n",
    "    if question[\"type\"] == \"yesno\":\n",
    "        # Generate yes/no answer\n",
    "        # the exact answer of each participating system will have to be either \"yes\" or \"no\".\n",
    "        params_chat = {\n",
    "            \"model\": model_name, # model name\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, \\\n",
    "                 research, and information retrieval in the biomedical domain.\"},\n",
    "                {\"role\": \"user\", \"content\": f\" {snippets}\\n\\n\\\n",
    "                 '{question['body']}'. \\\n",
    "                 You *must answer* only with lowercase 'yes' or 'no' even if you are not sure about the answer.\"}\n",
    "            ],\n",
    "            \"temperature\": 0.0, # randomness of completion\n",
    "            \"frequency_penalty\": 0.5, # discourage repetition of words or phrases\n",
    "            \"presence_penalty\": 0.3, # discourage new topics or entities\n",
    "        }\n",
    "        print(params_chat)\n",
    "\n",
    "        # Make request to chat completion\n",
    "        response_chat = requests.post(base_url + \"chat/completions\", headers=headers, json=params_chat)\n",
    "         # Check status code\n",
    "        if response_chat.status_code == 200:\n",
    "            # Parse response as JSON\n",
    "            data_chat = response_chat.json()\n",
    "\n",
    "            # Get the generated message from the chat completion result\n",
    "            generated_message = data_chat[\"choices\"][0][\"message\"][\"content\"]\n",
    "            print(\"\\ngpt response yesno:\")\n",
    "            print(generated_message)\n",
    "            generated_message = remove_punctuation_and_lowercase(generated_message)\n",
    "            exact_answer = generated_message\n",
    "        else:\n",
    "            raise Exception(f\"Error: {response_chat.status_code}\")\n",
    "    elif question[\"type\"] == \"factoid\":\n",
    "        # Generate factoid answer\n",
    "        # each participating system will have to return a json string array of up to 5 entity names (e.g., up to 5 names of drugs), numbers, or similar short expressions, ordered by decreasing confidence.\n",
    "        params_chat = {\n",
    "            \"model\": model_name, # model name\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, \\\n",
    "                 research, and information retrieval in the biomedical domain.\"},\n",
    "                {\"role\": \"user\", \"content\": f\" {snippets}\\n\\n\\\n",
    "                 '{question['body']}'. \\\n",
    "                 Answer this question by returning only a JSON string array of entity names, numbers, or similar short expressions that are an answer to the question, ordered by decreasing confidence. \\\n",
    "                 The array should contain at max 5 elements but can contain less. If you don't know any answer return an empty list. Return only this list, it must not contain phrases and **must be valid JSON**.\"}\n",
    "            ],\n",
    "            \"temperature\": 0.0, # randomness of completion\n",
    "            \"frequency_penalty\": 0.5, # discourage repetition of words or phrases\n",
    "            \"presence_penalty\": 0.1, # discourage new topics or entities\n",
    "        }\n",
    "        print(params_chat)\n",
    "\n",
    "        # Make request to chat completion\n",
    "        response_chat = requests.post(base_url + \"chat/completions\", headers=headers, json=params_chat)\n",
    "         # Check status code\n",
    "        if response_chat.status_code == 200:\n",
    "            # Parse response as JSON\n",
    "            data_chat = response_chat.json()\n",
    "\n",
    "            # Get the generated message from the chat completion result\n",
    "            generated_message = data_chat[\"choices\"][0][\"message\"][\"content\"]\n",
    "            print(\"\\ngpt response factoid:\")\n",
    "            print(generated_message)\n",
    "            \n",
    "            # Parse Json\n",
    "            # Extract the factoids from the generated message\n",
    "            factoids = json.loads(generated_message)\n",
    "            \n",
    "            wrapped_list = [[item] for item in factoids]\n",
    "            \n",
    "            exact_answer = wrapped_list\n",
    "        else:\n",
    "            raise Exception(f\"Error: {response_chat.status_code}\")\n",
    "    elif question[\"type\"] == \"list\":\n",
    "        # Generate list answer\n",
    "        # each participating system will have to return a single JSON string array of entity names, numbers, or similar short expressions, jointly taken to constitute a single answer (e.g., the most common symptoms of a disease). \n",
    "        # The returned list will have to contain no more than 100 entries of no more than 100 characters each.\n",
    "        params_chat = {\n",
    "            \"model\": model_name, # model name\n",
    "            \"messages\": [\n",
    "                {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, \\\n",
    "                 research, and information retrieval in the biomedical domain.\"},\n",
    "                {\"role\": \"user\", \"content\": f\" {snippets}\\n\\n\\\n",
    "                 '{question['body']}'. \\\n",
    "                 Answer this question by only returning a JSON string array of entity names, numbers, or similar short expressions that are an answer to the question (e.g., the most common symptoms of a disease). \\\n",
    "                 The returned list will have to contain no more than 100 entries of no more than 100 characters each. If you don't know any answer return an empty list. Return only this list, it must not contain phrases and **must be valid JSON**.\"}\n",
    "            ],\n",
    "            \"temperature\": 0.0, # randomness of completion\n",
    "            \"frequency_penalty\": 0.5, # discourage repetition of words or phrases\n",
    "            \"presence_penalty\": 0.1, # discourage new topics or entities\n",
    "        }\n",
    "        print(params_chat)\n",
    "\n",
    "        # Make request to chat completion\n",
    "        response_chat = requests.post(base_url + \"chat/completions\", headers=headers, json=params_chat)\n",
    "         # Check status code\n",
    "        if response_chat.status_code == 200:\n",
    "            # Parse response as JSON\n",
    "            data_chat = response_chat.json()\n",
    "\n",
    "            # Get the generated message from the chat completion result\n",
    "            generated_message = data_chat[\"choices\"][0][\"message\"][\"content\"]\n",
    "            print(\"\\ngpt response list answer:\")\n",
    "            print(generated_message)\n",
    "            \n",
    "            # Parse Json\n",
    "            # Extract the factoids from the generated message\n",
    "            list_answer = json.loads(generated_message)\n",
    "            \n",
    "            wrapped_list = [[item] for item in list_answer]\n",
    "            \n",
    "            exact_answer = wrapped_list\n",
    "        else:\n",
    "            raise Exception(f\"Error: {response_chat.status_code}\")\n",
    "    return exact_answer\n",
    "\n",
    "def generate_ideal_answer(question, articles, snippets):\n",
    "    # stub function for generating ideal answer\n",
    "    # a single paragraph-sized text ideally summarizing the most relevant information from articles and snippets\n",
    "    # The maximum allowed length of each \"ideal\" answer is 200 words.\n",
    "    # Each returned \"ideal\" answer is intended to approximate a short text that a biomedical expert would write to answer the corresponding question (e.g., including prominent supportive information)\n",
    "    params_chat = {\n",
    "        \"model\": model_name, # model name\n",
    "        \"messages\": [\n",
    "            {\"role\": \"system\", \"content\": \"You are BioASQ-GPT, an AI expert in question answering, \\\n",
    "             research, and information retrieval in the biomedical domain.\"},\n",
    "            {\"role\": \"user\", \"content\": f\" {snippets}\\n\\n\\\n",
    "             '{question['body']}'. \\\n",
    "             Answer this question by returning a single paragraph-sized text ideally summarizing the most relevant information \\\n",
    "             The maximum allowed length of the answer is 200 words. \\\n",
    "             The returned answer is intended to approximate a short text that a biomedical expert would write to answer the corresponding question (e.g., including prominent supportive information).\"}\n",
    "        ],\n",
    "        \"temperature\": 0.0, # randomness of completion\n",
    "        \"frequency_penalty\": 0.5, # discourage repetition of words or phrases\n",
    "        \"presence_penalty\": 0.7, # discourage new topics or entities\n",
    "    }\n",
    "    print(params_chat)\n",
    "\n",
    "\n",
    "    # Make request to chat completion\n",
    "    response_chat = requests.post(base_url + \"chat/completions\", headers=headers, json=params_chat)\n",
    "     # Check status code\n",
    "    if response_chat.status_code == 200:\n",
    "        # Parse response as JSON\n",
    "        data_chat = response_chat.json()\n",
    "\n",
    "        # Get the generated message from the chat completion result\n",
    "        generated_message = data_chat[\"choices\"][0][\"message\"][\"content\"]\n",
    "        print(\"\\ngpt response ideal anwer:\")\n",
    "        print(generated_message)\n",
    "        return generated_message\n",
    "    else:\n",
    "        raise Exception(f\"Error: {response_chat.status_code}\")\n",
    "    \n",
    "\n",
    "# Get the current timestamp in a sortable format\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "logfile_name = f\"{timestamp}_{model_name}_Grounded_PhaseB_log_file.json\"\n",
    "\n",
    "# Load the input file in JSON format\n",
    "with open('./BioASQ-task11bPhaseB-testset4.json', encoding='utf-8') as input_file:\n",
    "    data = json.loads(input_file.read())\n",
    "\n",
    "# Create an empty list to store the results\n",
    "results = []\n",
    "\n",
    "# Iterate over all questions\n",
    "offset = 0\n",
    "for idx, question in enumerate(data[\"questions\"]):\n",
    "    print(f\"\\n\\n{idx}\")\n",
    "    if idx < offset:\n",
    "        continue\n",
    "    # Determine the type of question\n",
    "    question_type = question[\"type\"]\n",
    "    print(f\"{question['body']}\\n\")\n",
    "\n",
    "    # Get the relevant articles and snippets\n",
    "    relevant_articles = question[\"documents\"]\n",
    "    relevant_snippets = question[\"snippets\"]\n",
    "    \n",
    "    # Ungrounded runs\n",
    "    relevant_snippets = []\n",
    "\n",
    "\n",
    "     # Generate the exact answer and ideal answer\n",
    "    try:\n",
    "        exact_answer = generate_exact_answer(question, relevant_articles, relevant_snippets)\n",
    "        ideal_answer = generate_ideal_answer(question, relevant_articles, relevant_snippets)\n",
    "    except Exception as e:\n",
    "        print(f\"Error processing question {idx}: {e}\")\n",
    "        time.sleep(5)  # Sleep for 5 seconds before retrying\n",
    "        exact_answer = generate_exact_answer(question, relevant_articles, relevant_snippets)\n",
    "        ideal_answer = generate_ideal_answer(question, relevant_articles, relevant_snippets)\n",
    "\n",
    "    # Create a dictionary to store the results for this question\n",
    "    question_results = {\n",
    "        \"id\": question[\"id\"],\n",
    "        \"type\": question_type,\n",
    "        \"body\": question[\"body\"],\n",
    "        \"ideal_answer\": ideal_answer,\n",
    "        \"exact_answer\": exact_answer,\n",
    "        \"documents\": relevant_articles,\n",
    "        \"snippets\": relevant_snippets\n",
    "    }\n",
    "    \n",
    "    # Add to logfile to continue after error with offset\n",
    "    append_to_logfile(logfile_name, json.dumps(question_results))\n",
    "\n",
    "    # Add the results for this question to the list of all results\n",
    "    results.append(question_results)\n",
    "\n",
    "# Create a dictionary to store the results for all questions\n",
    "output = {\n",
    "    \"questions\": results\n",
    "}\n",
    "\n",
    "# Get the current timestamp in a sortable format\n",
    "timestamp = datetime.datetime.now().strftime(\"%Y-%m-%d_%H-%M-%S\")\n",
    "\n",
    "# Prefix the output file name with the timestamp\n",
    "output_file_name = f\"{timestamp}_{model_name}_Grounded_PhaseB_output_file.json\"\n",
    "\n",
    "# Save the output to a file in pretty-formatted JSON format\n",
    "with open(f\"./Result/{output_file_name}\", \"w\") as f:\n",
    "    json.dump(output, f, indent=4)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "colab": {
   "provenance": []
  },
  "kernelspec": {
   "display_name": "base",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.4"
  },
  "vscode": {
   "interpreter": {
    "hash": "ad2bdc8ecc057115af97d19610ffacc2b4e99fae6737bb82f5d7fb13d2f2c186"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
